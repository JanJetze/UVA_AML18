{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "The goal of this assignment is to supply you with the **building blocks** of **neural networks** (NNs). In this notebook, we will cover the main aspects of NNs, such as **Backpropagation** and **Optimization Methods**. All mathematical operations should be implemented in **NumPy** only. \n",
    "\n",
    "The assignmnent consist of two parts:\n",
    "* *Part one: Blocks* - the place where the main **building blocks** of the NNs are implemented by you\n",
    "* *Part two: Experiments* - a playground to train the models\n",
    "\n",
    "### Note\n",
    "Some of the concepts below have not (yet) been discussed during the lecture. These will be discussed further during the next lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "* [0. Before you begin](#0.-Before-you-begin)\n",
    "\n",
    "Part one: Blocks\n",
    "\n",
    "* [1. Backpropagation](#1.-Backpropagation)   (mandatory)\n",
    "* [2. Dense layer](#2.-Dense-layer)   (mandatory)\n",
    "* [3. ReLU nonlinearity](#3.-ReLU-nonlinearity)   (mandatory)\n",
    "* [4. Sequential model](#4.-Sequential-model)\n",
    "* [5. Loss](#5.-Loss)   (mandatory)\n",
    "* [6. $L_2$ Regularization](#6.-$L_2$-Regularization)   (mandatory)\n",
    "* [7. Optimizer](#7.-Optimizer)\n",
    "* [8. Advanced blocks](#8.-Advanced-blocks)\n",
    "    * [8.1 Dropout](#8.1-Dropout)\n",
    "    * [8.2 MSE Loss](#8.2-MSE-Loss)\n",
    "    \n",
    "Part two: Experiments\n",
    "\n",
    "* [9. Circles Classification Task](#9.-Circles-Classification-Task)\n",
    "* [10. Digits Classification Task](#10.-Digits-Classification-Task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Before you begin\n",
    "\n",
    "To check whether the code you've written is correct, we'll use **automark**. For this, we created for each of you an account with the username being your student number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = 'your_student_number'\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far all your tests are 'not attempted'. At the end of this notebook you'll need to have completed all except 5 tests. The other five will be for next week. At the end of this notebook the output of `am.get_progress(username)` should match the example below.\n",
    "\n",
    "```\n",
    "---------------------------------------------\n",
    "| Your name / student number                |\n",
    "| your_email@your_domain.whatever           |\n",
    "---------------------------------------------\n",
    "| box_blur                 | not attempted  |\n",
    "| conv_matrix              | not attempted  |\n",
    "| dense_forward            | completed      |\n",
    "| dense_grad_W             | completed      |\n",
    "| dense_grad_b             | completed      |\n",
    "| dense_grad_input         | completed      |\n",
    "| dropout_forward          | completed      |\n",
    "| dropout_grad_input       | completed      |\n",
    "| flatten_forward          | not attempted  |\n",
    "| flatten_grad_input       | not attempted  |\n",
    "| hinge_forward            | completed      |\n",
    "| hinge_grad_input         | completed      |\n",
    "| l2_regularizer           | completed      |\n",
    "| maxpool_forward          | not attempted  |\n",
    "| mse_forward              | completed      |\n",
    "| mse_grad_input           | completed      |\n",
    "| relu_forward             | completed      |\n",
    "| relu_grad_input          | completed      |\n",
    "---------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division #You don't need to know what this is. \n",
    "import numpy as np #this imports numpy, which is used for vector- and matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paragraph is optional and **solely informational**. \n",
    "\n",
    "This notebook makes use of **classes** and their **instances** that we have already implemented for you. It allows us to write less code and make it more readable. If you are interested in it, here are some useful links:\n",
    "* The official [documentation](https://docs.python.org/3/tutorial/classes.html) \n",
    "* Video by *sentdex*: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8)\n",
    "* Antipatterns in OOP: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0)\n",
    "\n",
    "The interface of the current blocks is mostly inspired by **[Torch](http://torch.ch) / [PyTorch](http://pytorch.org)**. You can also take a look at the first implementation of [Keras](https://github.com/fchollet/keras/tree/37a1db225420851cc668600c49697d9a2057f098)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one: Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Backpropagation\n",
    "\n",
    "Neural networks consist of several layers. Each layer is a function of several parameters that we call weights: $h = f(x, w)$ where $h$ is the layer, $x$ is a vector of inputs and w is a vector of weights. \n",
    "In the neural network, the output of one layer is the input for the next layer. This means we can chain the different functions. The whole neural network $F$ then becomes a composition of different functions. \n",
    "$$\n",
    "F = f_k \\circ f_{k-1} \\circ \\dots \\ f_1\\\\\n",
    "h_1 = f_1(x, w_1)\\\\\n",
    "h_2 = f_2(h_1, w_2)\\\\\n",
    "\\dots \\\\\n",
    "\\dot{y} = f_k(h_{k-1}, w_k)\n",
    "$$\n",
    "In the above functions, $w_1$ and $w_2$ are different **weight vectors** that apply to the different layers $h_1$ and $h_2$. The weights of a neural network basically determine the effect certain outputs have on the next layer. (Please note: When searching for these terms on the internet, be aware that **weights** are sometimes called **parameters**, and $w$ is sometimes denoted as $\\theta$.) \n",
    "\n",
    "\n",
    "At the end of every neural network, there is a loss function. A loss function calculates for the performance of the Neural Network. The calculation of this score depends on the task at hand. For classification tasks the loss function would calculate the difference between prediction and the correct value. In this case the function is a summation of this difference for each data point. Calculating this difference can, again, be done in different ways. One example that we have discussed in class is the squared-loss for linear regression. (Here, the difference between predicted and correct classification is squared so positive and negative differences don't cancel eachother.) \n",
    "$$\\mathcal{L} = \\tfrac{1}{2}\\sum_{n = 1}^N (y_n - \\dot{y}_n)^2$$\n",
    "Here, $n$ denotes the different datapoints, $y_n$ and $\\dot{y}$ represent the correct and the predicted value for that data point respectively. \n",
    "\n",
    "\n",
    "\n",
    "The smaller the outcome of this loss function, the better the Neural Network predicts the data. Therefore, we concentrate on **minimizing the loss function** as a means for **training** the neural network. \n",
    "\n",
    "\n",
    "Training is done with [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). Another word for **gradient** is **derivative**. We use derivatives to update the weights of the neural network to make better predictions. \n",
    "The weights of the $k$-th layer are updated according to the following scheme:\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial w_k} \n",
    "$$\n",
    "* $\\partial f(x)/\\partial x$ means the partial derivative of $f(x)$ with respect to $x$. On the internet, derivatives are sometimes written as $f'(x)$, $df/dx$, $\\Delta f/x$ or $\\nabla f/x$. (Some of these notations have slightly different meanings, but you can ignore that for now.) \n",
    "* Hyperparameter $\\gamma$ is called the *learning rate* (You'll learn more about hyperparameters later. For now, the only thing you'll have to know is that the value of a hyperparameter is set by you.) \n",
    "* Note that $k$ denotes a layer and $n$ denotes a data point.\n",
    "\n",
    "\n",
    "The computation of $\\partial \\mathcal{L}/\\partial w_k$ is done using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_k}\n",
    "\\frac{\\partial h_k}{\\partial w_k} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_{k+1}}\n",
    "\\frac{\\partial h_{k+1}}{\\partial h_k}\n",
    "\\frac{\\partial h_k}{\\partial w_k} = \\dots\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, for each layer, we can calculate the following expressions: \n",
    "* $h_k = f_k(h_{k-1}, w_k)$ - the forward pass\n",
    "* $\\partial h_{k}/\\partial h_{k-1}$ - the partial derivative of the output with respect to the input\n",
    "* $\\partial h_{k}/\\partial w_k$ - the partial derivative of the output with respect to the parameters\n",
    "\n",
    "\n",
    "This whole process of updating weights by calculating the gradient is called [Backpropagation](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf). Click [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U) for a pretty good video explaining backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dense layer\n",
    "A dense Layer is the basic layer of a neural network. (Other terms for dense layer are fully-connected layer and multiplicative layer.) A dense layer transforms an input matrix of size `(n_objects, d_in)` to a matrix of size `(n_objects, d_out)` (where d stands for dimensions) by performing the following operation:\n",
    "$$\n",
    "H = XW + b\n",
    "$$\n",
    "Here $H$ represents the function of the dense layer, $X$ is the input matrix, $W$ is the weight matrix for this layer and $b$ is the bias. The bias $b$ is a vector. \n",
    "\n",
    "A more detailed version of this function is: \n",
    "$$\n",
    "H_{k,n} = \\sum\\limits_{i=1}^{d_{in}} X_{n,i}W_{i,k} + b_k\n",
    "$$\n",
    "where $n$ denotes again a single data object and $k$ the $k^{th}$ layer.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "You have a neural network of just 1 layer. The inputs are points in a 3D space and you want to classify this point as either $-1$ or $1$. \n",
    "You have $75$ objects in your training set. \n",
    "\n",
    "Therefore, $X$ has shape $75 \\times 3$. $H$ has shape $75 \\times 1$. Weight $W$ of the layer has shape $3 \\times 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input for a layer and producing its output is called a forward pass. Here, you'll implement the forward pass:  \n",
    "$$\n",
    "H = XW + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of a dense layer \n",
    "        np.array of size `(n_objects, b_out)`\n",
    "    \"\"\"\n",
    "    \n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check your first function. We set the matrices $X, W, b$:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "W = \\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "b = \\begin{bmatrix}\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And then compute \n",
    "$$\n",
    "XW = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-4 \\\\\n",
    "6 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "XW + b = \n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "-1 \\\\\n",
    "9 \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1, -1],\n",
    "                   [-1, 0],\n",
    "                   [1, 1]])\n",
    "\n",
    "W_test = np.array([[4],\n",
    "                   [2]])\n",
    "\n",
    "b_test = np.array([3])\n",
    "\n",
    "h_test =  dense_forward(X_test, W_test, b_test)\n",
    "print(h_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_forward, ['x_input', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll implement a backward pass. As decribed above, this is calculated with the gradient. To calculate the gradient, we'll use the chain rule: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_input, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of computing the gradient with respect to the input, we'll calculate the gradient with respect to the weights and to the bias: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial W} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial H}\n",
    "\\frac{\\partial H}{\\partial b} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to W parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to W parameter of the layer\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_W, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to b parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dense layer \n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to b parameter of the layer\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dense_grad_b, ['x_input', 'grad_output', 'W', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Class\n",
    "\n",
    "Here, we define a basic class for the dense layer. You will use this in the Experiments sections below. You don't need to know how this works; we implement it for you, but it is based on the functions you've written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        #Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        #initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "      \n",
    "    def forward(self, x_input):\n",
    "        self.output = dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ReLU nonlinearity\n",
    "\n",
    "The dense layer, from previous section, is linear. Combinging several linear (dense) layers is always equivalent to a single dense layer. Here is the mathematically proof for this: \n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = H_1W_2 + b_2\\\\\n",
    "H_2 = (XW_1 + b_1)W_2 + b_2 = X(W_1W_2) + (b_1W_2 + b_2) = XW^* + b^*\n",
    "$$\n",
    "\n",
    "\n",
    "For this reason, we're also going to need non-linear layers. Non-linear layers ($f$ in the following) are mostly element-wise and hold the following:\n",
    "$$\n",
    "H_1 = XW_1 + b_1\\\\\n",
    "H_2 = f(H_1)W_2 + b_2\\\\\n",
    "H_2 = f(XW_1 + b_1)W_2 + b_2 \\neq XW^* + b^*\n",
    "$$\n",
    "\n",
    "A popular example of a simple non-linear layer is **ReLU** (Rectified Linear Unit). ReLU doesn't have weights that can be optimized like a dense layer.\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "<img src=\"./src/relu.png\" width=\"500\">\n",
    "\n",
    "**Example**\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big(\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "\\Big) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, you will implement the forward pass and backward pass (gradient) for ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test forward pass for ReLU, see example above\n",
    "x_input = np.array([[1, -0.5],\n",
    "                    [0.3, 0.1]])\n",
    "\n",
    "print(relu_forward(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, relu_forward, ['x_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient. \n",
    "        Calculate the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss \n",
    "        with respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, relu_grad_input, ['x_input', 'grad_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = relu_forward(x_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential model\n",
    "In order to make the work with layers more comfortable, we create `SequentialNN` - a class, which stores all its layers and performs the basic manipulations. Again, this is for the experiments below and you don't need to know how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.training_phase = True\n",
    "        \n",
    "    def set_training_phase(self, is_training=True):\n",
    "        self.training_phase = is_training\n",
    "        for layer in self.layers:\n",
    "            layer.training_phase = is_training\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "            \n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "    \n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the simple neural network. It takes an input of shape `(Any, 10)` and passes it through `Dense(10, 4)`, `ReLU` and `Dense(4, 1)`. The output is a batch of size `(Any, 1)`. \n",
    "```\n",
    "  INPUT\n",
    "    |\n",
    "Dense(10, 4)\n",
    "    |\n",
    "   ReLU\n",
    "    |\n",
    "Dense(4, 1)\n",
    "    |\n",
    "  OUTPUT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SequentialNN()\n",
    "nn.add(Dense(10, 4))\n",
    "nn.add(ReLU())\n",
    "nn.add(Dense(4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the loss functions. Each loss should be able to compute its value and compute its gradient with respect to the input. We have implemented these functions (e.g. forward, backward) for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = 0.0\n",
    "        \n",
    "    def forward(self, target_pred, target_true):\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return np.zeros_like(target_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, you've seen a loss function for a linear regression. Here, you'll implement another sort of loss function used in classification: the [Hinge](https://en.wikipedia.org/wiki/Hinge_loss) loss function: \n",
    "$$ \n",
    "\\mathcal{L}(Y, \\dot{Y}) = \\frac{1}{N}\\sum\\limits_{n=1}^{N}\\max(0, 1 - \\dot{y}_n \\cdot y_n)\n",
    "$$\n",
    "\n",
    "* $N$ represents the number of data points\n",
    "* $\\dot{Y}$ and $Y$ are vectors of length $N$. $Y$ represents the true classes for your data points and $\\dot{Y}$ represents your prediction of the classes. \n",
    "* $\\dot{y}_n$ represents the predicted class of the $n$-th object. $\\dot{y}_n \\in {\\rm I\\!R}$\n",
    "* $y_n$ is the real class of this object. $y_n \\in \\{-1, 1\\}$\n",
    "\n",
    "This loss function is used in the SVM classifier. You will learn more about SVM in the lectures or click [here](https://www.youtube.com/watch?v=Y6RRHw9uN9o) for a helpful video. \n",
    "\n",
    "Now, you'll calculate the forward pass for the Hinge loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the value of Hinge loss \n",
    "        for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your forward pass below. Your output should be: `0.875`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test forward pass for Hinge\n",
    "target_pred = np.array([2,-5,3,0.5])\n",
    "target_true = np.array([-1,-1,1,1])\n",
    "print(hinge_forward(target_pred, target_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, hinge_forward, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should compute the gradient of the Hinge loss function with respect to its input. The output of this backward pass is a vector with the same shape as the input. To calculate the derivative of a vector, calculate the derivatives of its elements. Those derivatives become the elements of the derivative of the vector: \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{y}_1} \\\\ \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{y}_2} \\\\ \n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{y}_N} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, you'll implement the backward pass for the Hinge loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of Hinge loss with respect to its input\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################  \n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, hinge_grad_input, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hinge(Loss):\n",
    "    \n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = hinge_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return hinge_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. $L_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions update the weights of your model to improve your predictions. We do this by minimizing the loss function. However, up until now this loss function did not take into account the complexity of your model. Here we mean with complexity the number of parameters that your model stores. We do want to take complexity into account because complex models can perform poorly on test data, while performing excellent on train data. \n",
    "\n",
    "To penalize the complextity of the model, we introduce a regularizer. You'll learn more about regularizers in the lectures, but the general idea is that we take the values of the weights into account with the loss function. High values for weights are indicators of complexity. \n",
    "\n",
    "There are several ways of adding regularization to a model. We will implement [$L_2$ regularization](http://www.deeplearningbook.org/contents/regularization.html) also known as weight decay:\n",
    "\n",
    "The key idea of $L_2$ regularization is to add an extra term to the loss functions:\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|w\\|^2_2\n",
    "$$\n",
    "\n",
    "The part we added to the loss function is called the regularization function. \n",
    "* $\\lambda$ is named weight decay. It is a hyperparameter that determines the influence of the regularization to the outcome of the loss function. \n",
    "* $\\|w\\|^2_2$ is the squared [euclidian norm](https://en.wikipedia.org/wiki/Euclidean_distance) where $\\|w\\|^2_2 = \\|w_1\\|^2_2 + \\|w_2\\|^2_2 ... \\|w_k\\|^2_2$. \n",
    "This function in more detail becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$\n",
    "\n",
    "Because we use a different loss function, the updating of the weights is also slightly changed: \n",
    "\n",
    "$$\n",
    "w_m \\leftarrow w_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial w_m}\\\\\n",
    "\\frac{\\partial \\mathcal{L}^*}{\\partial w_m} = \\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\\\\n",
    "w_m \\leftarrow w_m - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial w_m} - \\lambda w_m\n",
    "$$\n",
    "\n",
    "Here, you'll implement the computation of $L_2$: \n",
    "$$\n",
    "L_2(\\lambda, [w_1, w_2, \\dots, w_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguments\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of different shapes\n",
    "    # Output\n",
    "        sum of the L2 norms of the input weights\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your forward pass below. Your output should be: `108.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the L2 regularizer\n",
    "weight_decay = 2\n",
    "weights = np.array([5,3,7,5,0.5])\n",
    "print(l2_regularizer(weight_decay, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, l2_regularizer, ['weight_decay', 'weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Optimizer\n",
    "\n",
    "We implement the optimizer to perform the updates of the weights. Here we have implemented two classes that you will need in the Experiments section.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    This is a basic class. \n",
    "    All other optimizers will inherit it\n",
    "    '''\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    '''\n",
    "        \n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * dw + self.weight_decay * w\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Advanced blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional section. If you liked the process of understanding neural networks by implementing them from scratch, here are several more tasks for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Dropout\n",
    "\n",
    "[Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) is a method of regularization. It can be interpreted as augmentation method. The key idea is to randomly drop some values of the input to avoid overfitting. Its behaviour is different during training and testing.\n",
    "\n",
    "![dropout](./src/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you should implement the method of calculating the binary mask. The binary mask has the same shape as the input. The mask could have the value 0 for the certain element with the probability `drop_rate` and it could have the value 1 with a probability of `1.0 - drop_rate`. \n",
    "\n",
    "**Note**, the $p$ from the paper linked above is `1.0 - drop_rate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_generate_mask(shape, drop_rate):\n",
    "    \"\"\"Generate binary mask \n",
    "    # Arguments\n",
    "        shape: shape of the input array \n",
    "            tuple \n",
    "        drop_rate: probability of the element \n",
    "            to be multiplied by 0\n",
    "            scalar\n",
    "    # Output\n",
    "        binary mask \n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the above-described operation of mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(x_input, mask, drop_rate, training_phase):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of the layer \n",
    "            np.array of size `(n_objects, d_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, d_in)`\n",
    "        drop_rate: probability of the element to be multiplied by 0\n",
    "            scalar\n",
    "        training_phase: bool either `True` - training, or `False` - testing\n",
    "    # Output\n",
    "        the output of the dropout layer \n",
    "        np.array of size `(n_objects, d_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dropout_forward, ['x_input', 'mask', 'drop_rate', 'training_phase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as usual, implement the calculation of the partial derivative of the loss function with respect to the input of a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_grad_input(x_input, grad_output, mask):\n",
    "    \"\"\"Calculate the partial derivative of \n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with \n",
    "            respect to the ouput of the dropout layer \n",
    "            np.array of size `(n_objects, n_in)`\n",
    "        mask: binary mask\n",
    "            np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with \n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, dropout_grad_input, ['x_input', 'grad_output', 'mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    \n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mask = 1.0\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        if self.training_phase:\n",
    "            self.mask = dropout_generate_mask(x_input.shape, self.drop_rate)\n",
    "        self.output = dropout_forward(x_input, self.mask, \n",
    "                                      self.drop_rate, self.training_phase)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x_input, grad_output):\n",
    "        grad_input = dropout_grad_input(x_input, grad_output, self.mask)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8.2 MSE Loss\n",
    "MSE (Mean Squared Error) is a popular loss for the regression tasks.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(Y, \\dot{Y}) = \\frac{1}{2N}\\sum\\limits_{n=1}^N(y_n - \\dot{y}_n)^2\n",
    "$$\n",
    "\n",
    "* $N$ - number of objects\n",
    "* $\\dot{Y}$ and $Y$ are the vectors of length $N$. \n",
    "* $\\dot{y}_n$ is the predicted target value of the $n$-th object. $\\dot{y}_n \\in {\\rm I\\!R}$\n",
    "* $y_n$ is the real target value of the $n$-th object. $y_n \\in {\\rm I\\!R}$\n",
    "* This loss function is used to train regressors.\n",
    "\n",
    "Let's implement the calculation of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of MSE loss\n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the value of MSE loss \n",
    "        for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, mse_forward, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should compute the gradient of the loss function with respect to its input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative \n",
    "        of MSE loss with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects,)`\n",
    "        target_true: ground truth - np.array of size `(n_objects,)`\n",
    "    # Output\n",
    "        the partial derivative \n",
    "        of MSE loss with respect to its input\n",
    "        np.array of size `(n_objects,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################     \n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, mse_grad_input, ['target_pred', 'target_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    \n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = mse_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, target_pred, target_true):\n",
    "        return mse_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's check the progress one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two: Experiments\n",
    "\n",
    "Seems like you've already implemented all the building blocks of the Neural Network. Now we will conduct several experiments.\n",
    "\n",
    "**Note:** These experiments will not be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Circles Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first task we created a dataset of 2 circles made out of dots from 2 different formulas. It is your task to predict for each dot which circle it belongs to. \n",
    "\n",
    "The main purpose of this task is to understand the **importance of network design and parameter tuning (number of layers, number of hidden units, etc.)**. At first, we will generate and visualize the data in following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 100\n",
    "phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "\n",
    "Y = np.concatenate([np.ones(N), -1.0 * np.ones(N)]).reshape((-1, 1))\n",
    "\n",
    "X = np.hstack([X1,X2]).T\n",
    "plt.scatter(X[:,0], X[:,1], c=Y.ravel(), edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have already written the code blocks, we will just call those functions and will train the network to classify the two circles. \n",
    "\n",
    "For this task we have provided the code for training and testing of the network in following blocks. Students are asked to design the network with different configurations  and observe the outputs.\n",
    "* **Single Layer Neural Network**\n",
    "* **Multiple Layer Neural Network**\n",
    "* **Different number of Hidden units**\n",
    "* **With and without activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training the network ##\n",
    "model = SequentialNN()\n",
    "\n",
    "###YOUR CODE FOR DESIGNING THE NETWORK ###\n",
    "# model.add(...)\n",
    "# model.add(...)\n",
    "\n",
    "loss = Hinge()\n",
    "weight_decay = 0.01\n",
    "sgd = SGD(model, lr=0.1, weight_decay=weight_decay)\n",
    "epochs = 100 # Number of times to iterate over all data objects\n",
    "\n",
    "model.set_training_phase(True)\n",
    "\n",
    "for i in range(100):\n",
    "    # get the predictions\n",
    "    y_pred = model.forward(X)\n",
    "    \n",
    "    # compute the loss value + L_2 term\n",
    "    loss_value = loss.forward(y_pred, Y) + l2_regularizer(weight_decay, model.get_params())\n",
    "    \n",
    "    # log the current loss value\n",
    "    print('Step: {}, \\tLoss = {:.2f}'.format(i+1, loss_value))\n",
    "    \n",
    "    # get the gradient of the loss functions\n",
    "    loss_grad = loss.backward(y_pred, Y)\n",
    "\n",
    "    # backprop the gradients\n",
    "    model.backward(X, loss_grad)\n",
    "    \n",
    "    # perform the updates\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing the network ##\n",
    "model.set_training_phase(False)\n",
    "y_pred = model.forward(X) > 0\n",
    "plt.scatter(X[:,0], X[:,1], c = y_pred.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Digits Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will implement a neural network for classification of hand written digits. You can use the blocks of code which you implemented in the first part of this assignment for completing this task.\n",
    "\n",
    "We will use **digits** dataset for this task. This dataset consists of 1797 8x8 images. Further information about the dataset can be found [here](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "# We load the dataset\n",
    "digits = sklearn.datasets.load_digits()\n",
    "\n",
    "# Here we load up the images and labels and print some examples\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "    plt.title('Training: {}'.format(label), y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will divide the images and labels data into two parts i.e. training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objects = digits.images.shape[0]\n",
    "train_test_split = 0.7\n",
    "train_size = int(n_objects * train_test_split)\n",
    "indices = np.arange(n_objects)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "train_images, train_targets = digits.images[train_indices], digits.target[train_indices]\n",
    "test_images, test_targets = digits.images[test_indices], digits.target[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the dataset are $8 \\times 8$ and each pixels in the image is an integer between ranging from 0 to 16. Before giving the images as input to the neural network we will reshape them to a 1-dimensional vector of 64 features as shown in the figure below.\n",
    "![in](./src/image_pixel_input.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((-1, 64))\n",
    "test_images = test_images.reshape((-1, 64))\n",
    "\n",
    "train_targets = train_targets.reshape(-1, 1)\n",
    "test_targets = test_targets.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic units of the neural network are perceptrons. A perceptron consists of a cell with atleast two inputs. The cell takes the inputs multiplied with weights and gives an output after computing the values. The basic diagram of a cell is shown below.\n",
    "![neuron](./src/neuron.png)\n",
    "For the image dataset which we will be using in this task the perceptron will have 64 inputs and 64 weights.\n",
    "![N_weights](./src/weights.png)\n",
    "As the digits dataset consists of 10 classes (0 to 9), in order to classifiy the images we will need 10 neurons for the prediction of the target class as shown in the image below. Each neuron will give an output and the output from the neuron with the highest value will be selected and that will be the predicted class.\n",
    "![NN](./src/design.png)\n",
    "Now, in order to perform classification for images you will use the functions which you implemented before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines of code we will train a complete Neural Network by giving the images as input and the labels as targets to the network. At first we will design the network by setting the parameters of the network by calling the *SequentialNN()* function. After that, we will forward propagate the inputs through the network and calculate the loss, the error between the predicted output and the target output. This loss will then be back propagated through the network. Finally the parameters of the network will be updated in the right direction to reduce the error of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE FOR TRAINING THE NETWORK###\n",
    "\n",
    "#Specify the input size for the network\n",
    "#specify the output size for the network\n",
    "#specify the inputs for the network\n",
    "#specify the outputs for the network\n",
    "num_input = None\n",
    "num_output = None\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "model = SequentialNN()\n",
    "# model.add(...)\n",
    "# model.add(...)\n",
    "# etc...\n",
    "\n",
    "loss = Hinge()\n",
    "weight_decay = 0.01\n",
    "sgd = SGD(model, lr=0.01, weight_decay=weight_decay)\n",
    "model.set_training_phase(True)\n",
    "\n",
    "for i in range(100):\n",
    "    # get the predictions\n",
    "    y_pred = model.forward(X)\n",
    "    \n",
    "    # compute the loss value + L_2 term\n",
    "    loss_value = loss.forward(y_pred, Y) + l2_regularizer(weight_decay, model.get_params())\n",
    "    \n",
    "    # log the current loss value\n",
    "    print('Step: {}, \\tLoss = {:.2f}'.format(i+1, loss_value))\n",
    "    \n",
    "    # get the gradient of the loss functions\n",
    "    loss_grad = loss.backward(y_pred, Y)\n",
    "    # backprop the gradients\n",
    "    model.backward(X, loss_grad)\n",
    "    \n",
    "    # perform the updates\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the network should be tested on test data; images in this task. During the test time unlabeled inputs are given to the network and by using the trained weights from the training cycle of the network the ouput classes for the unlabeled inputs are predicted. The figure below shows the difference between the training and the testing of the network.\n",
    "![test](./src/Test.PNG)\n",
    "In the following cell implement the code for testing the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the network \n",
    "###YOUR CODE FOR TESTING THE NETWORK  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further practice and advanced experiments you can use MNIST dataset for the classification of hand written digits. It is a commonly used image dataset for testing machine learning algorithms. You can load this dataset by using the code from the file *data_utils* in week_1 folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
